{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this script uses data that I collect using the Twitter streaming API to analyze \n",
    "# where tweets are coming from and the sentiment of them. Preliminary figures show some \n",
    "# cool temporal dynamics and perhaps some cultural differences.\n",
    "# Full dataset is ~30Gb and ~180 million tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Point\n",
    "import numpy as np\n",
    "import datetime\n",
    "import glob\n",
    "import time\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading 20210615-102758_twitter.db\n",
      "loading twitter_fall.db\n",
      "loading 20210703-133946_twitter.db\n",
      "loading 20210613-104941_twitter.db\n",
      "loading 20210703-122204_twitter.db\n",
      "loading 20210704-115426_twitter.db\n",
      "loading 20210615-172857_twitter.db\n",
      "loading twitter_spring.db\n",
      "loading 20210616-180158_twitter.db\n",
      "loading 20210616-090825_twitter.db\n",
      "loading twitter_geo3.db\n",
      "loading twitter_2.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-bcf873d71487>:25: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  df = pd.concat(data)\n"
     ]
    }
   ],
   "source": [
    "# load multiple sqlite DBs w/ scraped tweets\n",
    "ts = np.vectorize(datetime.datetime.fromtimestamp)\n",
    "os.chdir('/media/data/twitter/')\n",
    "if not os.path.isfile('/media/data/twitter/twitter_data_clean.csv'):\n",
    "    dbs = glob.glob('*db')\n",
    "    data = []\n",
    "    for db in dbs:\n",
    "        print('loading ' + db) # I split these files up so any one isn't too big\n",
    "        conn = sqlite3.connect(db)\n",
    "        try:\n",
    "            df = pd.read_sql(\"select unix, sentiment, loc_source, longitude, latitude, location from sentiment\", conn) #  unix, sentiment, loc_source, longitude, latitude, location\n",
    "        except:\n",
    "            df = pd.read_sql(\"select unix, sentiment, longitude, latitude from sentiment\", conn) #  unix, sentiment, loc_source, longitude, latitude, location\n",
    "            # if we don't have location info, or lat/long.. we can drop some rows \n",
    "            # an older version of my scraper didn't save loc_source but still ~1/50 tweets has lat/long data\n",
    "            df = df.dropna()\n",
    "            df['locatin'] = ''\n",
    "            df['loc_source'] = ''\n",
    "        dates = ts(df.unix.values/1000)\n",
    "        df.index = dates\n",
    "        conn.close()\n",
    "        data.append(df)\n",
    "\n",
    "    df = pd.concat(data)\n",
    "    del data, dates\n",
    "    df = df.sort_index()\n",
    "    df.to_csv('twitter_data_clean.csv')\n",
    "else:\n",
    "    df = pd.read_csv('twitter_data_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started with:  180955104  missing locations\n"
     ]
    }
   ],
   "source": [
    "# we are going to attempt to geocode unlabelled tweets by the 'location' listed for users that share this.. fully aware that this won't be correct (or a real place) for many\n",
    "from geopy.geocoders import Nominatim\n",
    "geolocator = Nominatim(user_agent=\"twitter_geo\")\n",
    "\n",
    "print('started with: ', sum(df.latitude.isnull()), ' missing locations')\n",
    "\n",
    "n = 10 # n is the number of places we will try to get lat/long coord for, sorted by most common places\n",
    "locs = df['location'].value_counts()[:n].index.tolist()\n",
    "\n",
    "lon = int(np.where(df.columns=='longitude')[0])\n",
    "lat = int(np.where(df.columns=='latitude')[0])\n",
    "\n",
    "for loc in locs:\n",
    "    try:\n",
    "        lo  = geolocator.geocode(loc,timeout=5) # get coords for a place\n",
    "        idx = np.where(df.location==loc)\n",
    "        for ind in idx[0]:\n",
    "            df.iat[ind,lon] = lo.longitude\n",
    "            df.iat[ind,lat] = lo.latitude   \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "print('ended with: ', sum(df.latitude.isnull()), ' missing locations')\n",
    "print('working with: ', sum(df.latitude.notnull()), ' locations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's only keep the complete data\n",
    "df = df[df['latitude'].notnull()]\n",
    "print(len(df))\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geometry = [Point(xy) for xy in zip(df['longitude'], df['latitude'])]\n",
    "gdf = gpd.GeoDataFrame(df, geometry=geometry)   \n",
    "\n",
    "# #this is a simple map that goes with geopandas\n",
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "gdf.plot(ax=world.plot(figsize=(20, 10),edgecolor='k',color='w'), marker='o', cmap='bwr', markersize=5,vmin=-1,vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a geoDataFrame using lat/long of tweets\n",
    "places = gdf\n",
    "places[\"geometry\"] = gdf.apply(lambda row: Point(row[\"longitude\"], row[\"latitude\"]), axis=1)\n",
    "places = gpd.GeoDataFrame(places, geometry=\"geometry\")\n",
    "places.crs = {\"init\": \"epsg:4326\"}\n",
    "\n",
    "# Load the countries polygons\n",
    "country_shapes = world[['geometry', 'iso_a3']]\n",
    "country_names = world[['name', 'iso_a3']]\n",
    "countries = world[['geometry', 'name']]\n",
    "countries = countries.rename(columns={'name':'country'})\n",
    "\n",
    "# join tweet lat/long with country shapes (i.e. borders)\n",
    "result = gpd.tools.sjoin(places, countries, how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the overall median and hourly average sentiment values for each country, as heatmaps\n",
    "sentAvg = []\n",
    "sentLastHr = []\n",
    "\n",
    "for country in world['name']:\n",
    "    sentAvg.append(result.loc[result['country']==country]['sentiment'].median())\n",
    "    sentLastHr.append((result.loc[result['country']==country]['sentiment'].last('4h').mean()-result.loc[result['country']==country]['sentiment'].mean())/result.loc[result['country']==country]['sentiment'].std())\n",
    "    \n",
    "world['sentiment'] = sentAvg\n",
    "world['sentiment_lastHr'] = sentLastHr\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=1,figsize=(30,12))\n",
    "\n",
    "world.dropna().plot(ax=axes[0],column='sentiment_lastHr',cmap='coolwarm',legend=False,vmin=-.25,vmax=.25) \n",
    "axes[0].set_title('last hour')\n",
    "\n",
    "world.dropna().plot(ax=axes[1],column='sentiment',cmap='coolwarm',legend=False,vmin=-.35,vmax=.35) \n",
    "axes[1].set_title('total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at temporal dynamics\n",
    "result[result['country']=='United States of America']['sentiment'].rolling('200S',min_periods=10).mean().plot(marker='.')\n",
    "plt.title(('United States of America',len(result.loc[result['country']=='United States of America'])))\n",
    "# plt.xlim((datetime.datetime(2021,7,3,5,48),datetime.datetime.now()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[result['country']=='United States of America']['sentiment'].rolling('200S',min_periods=10).mean().plot(marker='.')\n",
    "plt.title(('United States of America',len(result.loc[result['country']=='United States of America'])))\n",
    "# plt.xlim((datetime.datetime(2021,7,3,5,48),datetime.datetime.now()))\n",
    "plt.xlim((datetime.datetime(2020,6,4,1,48),datetime.datetime(2020,6,6,5,48)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(result['latitude'].dropna().index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    ''' by Fred Cirera,  https://stackoverflow.com/a/1094933/1870254, modified'''\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f %s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f %s%s\" % (num, 'Yi', suffix)\n",
    "\n",
    "for name, size in sorted(((name, sys.getsizeof(value)) for name, value in locals().items()),\n",
    "                         key= lambda x: -x[1])[:10]:\n",
    "    print(\"{:>30}: {:>8}\".format(name, sizeof_fmt(size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
